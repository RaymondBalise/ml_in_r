---
title: "Machine Learning Modeling with the R Tidymodels Ecosystem"
subtitle: "Who Knows, Who Cares, Why Bother..."
author: 
  - name: "Raymond Balise PhD"
    affiliation: 
      - "Univerity of Miami, Biostatistics"
  - name: "Franciso Cardozo (almost PhD)"
    affiliation: 
      - "Prevention Science Community Health"
date: "`r Sys.Date()`"
format:
    revealjs:
      theme: [default]
      fontsize: 2em
knitr:
    opts_chunk:      ########## set global options ############
        collapse: true # keep code from blocks together (if shown)
        echo: false    # don't show code
        message: true  # show messages
        warning: true  # show warnings
        error: true    # show error messages
        comment: ""    # don't show ## with printed output
        R.options:    
            digits: 3    # round to three digits
---

```{css}
.scrolling {
  max-height: 500px;
  overflow-y: auto;
}

```

```{r}
#| label: tidyverse
#| echo: false

library(conflicted)
conflict_prefer("filter", "dplyr", quiet = TRUE)
conflict_prefer("lag", "dplyr", quiet = TRUE)

suppressPackageStartupMessages(library(tidyverse))

# suppress "`summarise()` has grouped output by " messages
options(dplyr.summarise.inform = FALSE)

suppressPackageStartupMessages(library(tidymodels))
tidymodels_prefer()

```

```{r}
#| label: load-data

```

# The Plan

::: columns
::: {.column style="font-size: .8em;"}
-   Thoughts on:
    -   Learning
    -   Different Views of Statistics
    -   Evaluating Success
    -   Ways to Measure Success
    -   R Modeling
-   Methods to Make Predictions
    -   Similar Things (kNN)
    -   Traditional Equations
        -   OLS & Logistic Regression
    -   Classification Trees
        -   CART
        -   Random Forests
:::

::: {.column style="font-size: .8em;"}
-   How to Do (tidy) Modeling in R
    -   Import
    -   EDA
    -   Select Vars
    -   Train/Test
    -   Recipe (Formula & Steps)
    -   Specify Model
    -   Specify Resampling
    -   Tune
    -   Select Parameters
    -   Evaluate
    -   Finalize
:::
:::

# Thoughts on Learning

## Terms Confused in the Media

-   Artificial Intelligence \> Machine Learning \> Neural Networks

-   "Artificial Intelligence", 1956

    -   refers to when a [machine observes and responds]{style="color:red;"} to its environment

-   "Machine Learning", 1959

    -   refers to [algorithms with feedback loops]{style="color:red;"} that allow learning from experience

-   "Neural Networks", 1958

    -   subset of machine learning methods that feature [many nodes]{style="color:red;"} (individual, generalized linear models) that [work together]{style="color:red;"} to answer a question

## Human Brain and Other Black Box Learners

-   Some machine learning algorithms are like the human brain. [We know the fine details]{style="color:red;"} of what they are doing but [we don't understand how they actually work]{style="color:red;"}.
    -   Where does self-awareness come from in the brain?
-   Many ML methods are black boxes.
    -   Excellent predictions
    -   Neural networks [link together "nodes"]{style="color:red;"}
    -   Support vector machines use [multidimensional surfaces]{style="color:red;"}
-   *State of the art* methods shine a light into the black box.

## Do you care [how and why]{style="color:red;"} it works?

-   I ask myself, "Can I explain how and why this works to my mom?"
    -   Decision Trees - very easy to explain
    -   k-nearest-neighbors - needs a pen and the back of an envelope\
    -   Logistic Regression - needs a whiteboard and a long conversation
    -   Random Forests - difficult but doable
    -   Neural Networks - very difficult to explain
-   Balance the [quality]{style="color:red;"} of its predictions vs. the need to understand how it works.

## How do you know when it is going to fail?

-   If you don't know the role of the predictors you don't know how the model will break down.
-   Are subsets of the population systematically incorrect?
    -   If an algorithm says Black people will relapse that is a problem.
    -   If an algorithm guesses wildly for Black people that is a problem.
-   What do you do if the quality of predictions are systematically worse for a race group?

## Do I need to assess the predictors?

-   Some methods predict extremely well but they tell you little (or nothing) about if/how the individual predictors influence the outcome.

-   Different methods tell you different things:

-   Does having more of a predictor lead to more of the outcome?

    -   Which predictors are the most important (but you have no idea how they work)
    -   Which predictors matter (but you have no idea how they work)
    -   You know nothing about which predictors work

## Learning vs. Cramming for an Exam

-   How does a *party monster* pass a class?
    -   Studies previous test questions
    -   Echoes back the same answers
-   How does an excellent student pass a class?
    -   Looks at [many examples]{style="color:red;"}, what features make a system work
    -   Generalizes predictors to [guess a target]{style="color:red;"}

## Types of Learning Problems

Machine learning methods can handle:

-   Classification
    -   Is there HIV in blood?
    -   Does someone have AIDS?
-   Numeric Prediction
    -   Viral load
    -   Days until leaving care
-   Pattern Detection
    -   What drugs are frequently used together
-   Clustering
    -   What groups of people will respond to an intervention

## Supervised vs. Unsupervised Learning

-   How do you recognize a dog?
    -   As a young child, with supervised learning, you had a caregiver provide you feedback over and over until you knew a dog from a cat from a cow.
    -   We are getting good at building machines to do this.
-   As a baby, how did you learn to recognize your mom?
    -   With unsupervised learning you automatically recognized similarities that described your mom. You just noticed some "features" always occurred together. Colored shapes, smells, sounds and touches happened together and you noticed the pattern.
    -   We have had limited success in doing this.
-   Where do superstitions come from?
    -   With semi-supervised learning, you may get feedback on a few trials, then you make a guess and confirm what you think.

## Types of Problems -- Unsupervised Learning

-   No Outcomes
    -   64 cancer cell lines with 6,830 expression markers
    -   Find similar types of cells.... Specify you want 4 groups ![+](images/clustered_cells.png){fig-align="center" width=75%}
    -   After the fact, you can look to see if the groups make sense.
    -   Here the cells actually came from 14 cancer cell lines.

::: {.footer style="font-size: .2em;"}
Image from: An Introduction to Statistical Learning: with Applications in R.

Copyright 2013 Springer Science+Business Media
:::

## Inductive Logic

Machine learning uses inductive (not deductive) logic to learn.

-   You use [deductive]{style="color:red;"} logic for doing [proofs]{style="color:red;"} and derivations.
    -   Start with a set of truths primitives and get to a guaranteed correct answer.
-   Machine learning methods and people typically use [inductive]{style="color:red;"} logic.
    -   [Observe a pattern]{style="color:red;"} over and over and use it to guide you toward an answer.

# Different Views of Statistics

## Medicine vs. Statistics vs. Machine Learning

-   Medicine cares most about [central tendency]{style="color:red;"}.
    -   Doctors care about what is a normal lab value (the mean).
-   Statistics cares about [variability]{style="color:red;"}.
    -   Statisticians care about defining the range of normal values across samples (the standard error).
-   Machine learning cares about [prediction]{style="color:red;"}.
    -   Business leaders care about predicting which things will sell.

## $\beta$ or $\hat{y}$


$$
\hat{y} = \alpha + \beta_1x_1 + \beta_2x_2 + \cdots + \varepsilon 
$$

+ Most statistics classes focus on estimating betas
$$
\text{Weight} = \text{Baseline} + \text{Height in Inches} \times \beta_1 + \text{Is Male} \times \beta_2
$$

+ Machine learning classes focus on estimating y-hat.

$$
\text{A Person's Weight} = f(\text{Height in Inches}), f(\text{Is Male})
$$

## Beyond Euclidean Distance

+ [Common statistical]{style="color:red;"} methods look at [squared error]{style="color:red;"}.
+ The difference between predictions and outcomes can be [measured in many ways]{style="color:red;"}.
+ If you think of data points living in high dimensional space, you can easily use other metrics which describe [differences in direction]{style="color:red;"}.
    + Cosine similarity
    + Chebychev distance
    + Mahalanobis distance
    + Hamming distance

## When loss is **bad**

+ If you need to make a [life and death prediction]{style="color:red;"}, you don't want to limit yourself to linear models and squared errors.  
+ You want to be comfortable with the idea of exploring [different loss functions]{style="color:red;"} and use the algorithm that gets you the [best possible prediction]{style="color:red;"}.

## Why not look at nonlinear models?

+ !@#$ the betas….
+ There are popular algorithms that give better predictions, but you will gain relatively little or [no insight into how]{style="color:red;"} the predictors are driving the results.
+ Non-linear models:
   + kNN 
   + SVM
   + Neural Networks
   + Random Forests

# How to Measure Success

## When I was in School 

+ It was about p-values and residual diagnostics

## Bias Variance Trade-off

:::: {.columns}

::: {.column}

+ RED:
  + Low Variance - because new sin wave data would have a similar model
  + High Bias - because the points are poorly modeled
  + Underfit - It does not follow the pattern well!

:::

::: {.column}

+ BLUE
  + High Variance – because small changes in the data will make a different model
  + Low Bias – because it fits the data well
  + Overfit - It follows the pattern of THESE data too well.

:::

::::

![](images/biasVariance.png){fig-align="center" width=100%}

# Creating Models in R

(or ... thoughts on why Ray says R is user hostile)


## The Insanity of Many Formula Interfaces $_1$

### The *Formula* Interface 

::: {.smaller}

`Y ~ X` where we say "Y is a function of X".

```{r 2-3-1a, eval=FALSE, echo=TRUE}
# Variables and interaction
model_fn(Sale_price ~ Neighborhood + Year_Sold + Neighborhood:Year_Sold, 
         data = ames)

# Shorthand for all predictors
model_fn(Sale_price ~ ., data = ames)

# inline function /transformations
model_fn(log10(Sale_price) ~ ns(Longitude, df = 3) + ns(Latitude, df = 3),
         data = ames)
```


- You [can't nest]{style="color:red;"} in-line modeling functions (like `pca`)
```{r eval=FALSE, echo=TRUE}
(model_fn(y ~ pca(scale(x1), scale(x2), scale(x3)), data = df))
```
-  All the model matrix calculations happen at once and [can't be recycled]{style="color:red;"} when used in a model function
-   For very wide datasets, [extremely inefficient]{style="color:red;"}
-   [Multivariate]{style="color:red;"} outcomes are clunky and [inelegant]{style="color:red;"}.

:::

## The Insanity of Many Formula Interfaces $_2$

### The *Non-Formula (x, y)* Interface 

These functions have separate arguments (different objects) for the predictors and the outcomes(s):

```{r 2-3-1b, eval=FALSE, echo=TRUE}
features <- c("Year_sold", "Longitude", "Latitude")
model_fn(x = ames[, features], y = ames$Sale_Price)
```

+ Efficient calculations 
+ Inconvenient if you have transformations, factor variables, interactions, or any other operations to apply to the data prior to modeling.

## The Insanity of Many Formula Interfaces $_3$

### *Variable Name Specification* Interface

All the data combined in one data set but the features and responses with character strings. 

```{r 2-3-1c, eval=FALSE, echo=TRUE}
model_fn(x = c("Year_sold", "Longitude", "Latitude"), y = "Sale_Price", 
         data = ames.h2o)
```

Similar issues non-formula interface

## The Horror of Many Function Arguments

The arguments are not the same across different functions:

```{r 2.3.2, eval=FALSE, echo=TRUE}
lm_lm <- lm(Sale_Price ~ . data = ames)
lm_glm <- glm(Sale_Price ~ . data = ames, family = gaussian)
lm_caret <- train(Sale_Price ~ . data = ames, method = lm)
```

<br/>
```{r echo = FALSE}
`Table 2.1` <- data.frame(
  Algorithm  = c("Linear discriminant analysis", "Generalized Linear model", 
                 "Mixture discriminant analysis", "Decision tree", 
                 "Random forest", "Gradient boosting machine"),
  Package = c("MASS", "stats", "mda", "rpart", "ranger", "gbm"), 
  Code = c("`predict(obj)`", 
           "`predict(obj, type = 'response')`", 
           "`predict(obj, type = 'posterior')`", 
           "`predict(obj, type = 'prob')`", 
           "`predict(obj$predictions)`",
           "`predict(obj, type = 'response', n.trees)`"))

suppressMessages(library(kableExtra))
kable(`Table 2.1`) %>% column_spec(1:3, background = "white")
```


# Tidy Modeling in R

## The `caret` Package

+ The `caret` package offered a unified interface to deal with inconsistencies.  

+ `caret` (and `mlr`) dominated the R machine learning landscape before the 2019/2020. 
+ You will see examples using `caret` all over the web.

+ Now it is part of `tidymodels` ecosystem.  Try to find the `tidymodels` (`parsnip`) analogous code.

## Core Stickers

![](images/tidymodelsStickers.png)

:::{.notes}
composable = components that can be selected and assembled in various combinations

consistent = you know what a function will return (data will change in a known way)

extensible = internal structure and data-flow are minimally or not affected by new or modified functionality
:::

##

![](images/role-2.png)

## 

![](images/role-1.png)

##

![](images/role.png)

## An (overly) Simple Analysis - with No Tuning

+ Use `rsample`] to split your data into training and test
+ Use `recipies`] to specify your outcome, predictors, transformations and subject ID variables (if any)
  + Use `prep()`,  `bake()`, ~~`juice()`~~ to look at the data before modeling
+ Use `parsnip` to specify your model
  + Logistic regression does not require any tuning
  + Other models will badly overfit the data
+ Use `workflows` to glue together the recipe and model then fit the model

+ Use `yardstick` to describe the model quality

## `recipies` - Preprocessing at its Best

+ With `tidyverse` you can preprocess your entire dataset.

+ When [tuning]{style="color:red;"} ML methods you will want to preprocess ***subsets*** of your data separately.

+ If your preprocessing involves imputing missing values or normalizing/standardizing you want to do this work on ***each subset separately***.

+ The `recipes` package makes it simple to preprocess the subsets.

## What steps will `recipies` do?

+ Imputation
+ Univariate Transformations
+ Discretize - Make continuous categorical
+ Dummy Coding and Encoding
+ Normalize
+ Filter
+ Row operations
+ Add Interactions
+ Multivariate Transformations

So much more: <https://recipes.tidymodels.org/reference/index.html#section-basic-functions>

## Important `step_*()` Functions in `recipies`$_1$

+ Univariate Transformation
  + `step_log()` - base e by default
  + `step_YeoJohnson()` - make distribution as Gausian as possible
+ Dummy Coding and Encoding
  + `step_date()` - split date into month, day, year variables
  + `step_dummy()` - make dummy or one hot indicators
  + `step_other()` - group rare categories
+ Normalize
  + `step_normalize()` - center and scale numeric data

## Important `step_*()` Functions in `recipies`$_2$

+ Filter
  + `step_corr()` - remove highly correlated ([be careful]{style="color:red;"})
  + `step_nzv()` - remove variables little variability (near zero variance)
  + `step_zv()` - remove constants (zero variance)
+ Multivariate Transformations
  + `step_pca()` - extract principle components
  + `step_pls()` - extract partial least squares

## Using `recipies` Without a Workflow

:::{.smaller}

I typically use the `recipies` package with `workflow` package to seamlessly integrate preprocessing and modeling but you can look at just the results of the preprocessing. Using `recipies` alone [you can see the transformed data with the `juice()` or `bake()` functions like this]{style="color:red;"}:

:::

```{r eval = FALSE, echo=TRUE}
# these are instructions
the_recipe <- 
  recipe(isDead ~ ., data = training_data) %>%  # use all the variables to predict
  update_role(subject, new_role = "ID") %>%  # keep subject number but don't model
  step_other(country, language = 0.01) %>%  # bin rare countries & languages as other
  step_date(date, features = c("year")) %>%  # extract year only
  step_rm(date) %>%  # drop original date
  themis::step_downsample(isDead)  # make get rid of some of the living people

# assign role, make other categories, convert date, figure out the down sample %
the_prep <- prep(the_recipe)  # this is `large_recipe object`

# apply the prepared recipe - a good idea
the_baked <- bake(the_prep, data = NULL)  # this is a `data set` the processed training_data

# apply the recipe to new data - sometime useful for debugging
the_bake <- 
  bake(the_prep, 
       new_data = weird_new_data) # this is a `data set` with the processed weird_new_data
```

## The `recipies` Package Details

::::: {.smaller}

You can specify that steps apply to specific types of variables using these functions inside of a `step_*()` function:

:::: {.columns}

::: {.column width="60%"}

+ `has_role(match = "predictor")`
+ `all_numeric_predictors()`
+ `all_nominal_predictors()`
+ `has_type(match = "numeric")`

:::

::: {.column width="40%"}

+ `all_predictors()`
+ `all_outcomes()`
+ `all_numeric()`
+ `all_nominal()`
:::
::::

:::::

```{r, eval=FALSE, echo=TRUE}
recipe(outcome ~ ., data = my_data) %>% 
  step_dummy(all_nominal()) %>%  # dummy code all character and factor variables
  step_zv(all_predictors()) %>%  # drop numeric variables with zero variance
  step_normalize(all_predictors())  # normalize the predictors
```


# The Process 

The goal is to have your predictions approximate the truth.


## If you had billions of dollars … or  … If you could be omnipotent for a day

+ You would do a census of everybody.

  + In theory, there is no sampling bias and you know the truth.

## If you had many millions of dollars… 

+ You would get a HUGE sample and use it for making predictions

+ You would check your results on another very large sample

## If you had a lot of money…

+ You would get the biggest sample you could afford.

+ Put the data in random order.

+ Build you model on 3/4 of the data.  
  + Model [training]{style="color:red;"} data 

+ Once you had the final model you would test it on the other 1/4.
  + Model [test]{style="color:red;"} data.	

## Train and Test

:::: {.columns style='display: flex !important; height: 90%;'}

::: {.column}

-   *Training set*: choose features, build models, train our algorithms, tune hyperparameters, compare models.

-   *Test set*: estimate an unbiased assessment of the model's performance, its *generalization error*. You use the test data [only once]{style="color:red;"}!
  
Typical recommendations for splitting your data into training-test splits include 60% (training) - 40% (test), 70%-30%, or 80%-20%

:::

::: {.column style='display: flex; justify-content: center; '}


```{r, echo = FALSE, fig.cap=""}
knitr::include_graphics("https://bradleyboehmke.github.io/HOML/images/data_split.png")
```

:::

::::

::: {.footer style="font-size: .2em;"}

Image from https://bradleyboehmke.github.io/HOML/images/modeling_process.png

:::


## How to Split for Training and Test

The two most common ways of splitting data include [*simple random sampling*]{style="color:red;"} and [*stratified sampling*]{style="color:red;"}.

+ *Simple random sampling*:
  + [Ignore outcomes]{style="color:red;"} while randomly putting the people into the test set. 
  
  + This does not control for any data attributes, such as the distribution of your response/outcome variable $(Y)$.

+ *Stratified sampling*: 
  + Makes sure training and test sets have [similar outcome distributions].
  
  + I use it (always).  It is important when there is a class imbalance imbalance in the outcomes (e.g., 90% of patient fail treatment and 10% respond).

## Dealing with Class Imballances

*Down-sampling*: reducing the size of the abundant class(es) to match the frequencies rare classes. 

*Up-sampling*: new rare samples are generated by using repetition or bootstrapping.

*Synthetic Minority Over-Sampling Technique, or SMOTE* over- and under-sampling.


## Traditional Models

+ Splitting data into two groups makes perfect sense if you are doing ordinary least square regression or logistic regression.

  + Manual or automatic procedures
  
+ You see the results on the new data.
  + Does the logistic guess the correct category.
  
  + Does the ordinary least squares regression model come up with about the right number for the outcome.

## Modern Methods

+ Building models with modern methods is tricky because they have parameters you need to [tune]{style="color:red;"}.

  + For example, k nearest neighbors makes an outcome guess for a new person based on some similar people.  [How many similar people should you use to get the best prediction?]{style="color:red;"}
  
  + Other algorithms (like CART or LASSO/Elastic Net) allow you to specify a "penalty" that each predictor needs to overcome to be included in a model. [How big a penalty gives the best prediction?]{style="color:red;"}
      
      + A large penalty would require a predictor to have a whopping huge improvement in the prediction (i.e., the residuals/the loss function). You get a simple model.
      
      + A tiny penalty would allow you to add more predictors.  You get a complex model.  

## Split the Training Data $_1$

To check the impact of the tuning parameters you build a model on [part of the training data]{style="color:red;"}.  You have two options:

1) Cross-validate:

  + Put the training data in random order.
  
  + Set aside the [first $10^{th}$]{style="color:red;"} of the training data and call it [validation/evaluation]{style="color:red;"} data.  Build on the remaining 90% and evaluate/validate with the evaluation/validation 10%.
  
  + Put back the first 10th and set aside the [second $10^{th}$]{style="color:red;"} of the training data and call it [validation/evaluation]{style="color:red;"} data.  Build on the remaining 90% and evaluate/validate with the evaluation/validation 10%.
  
  + Repeat on all $10^{th}$s.  Every person is used to help validate.

## Split the Training Data $_2$

2) Bootstrap

+ Note the size of your sample.  Call the number of people in the sample [N]{style="color:red;"}.
  
+ Take a [random sample with replacement]{style="color:red;"} of size [N]{style="color:red;"}. 
  + The process of building random samples of size N is called "the bootstrap" or ["bootstrapping"]{style="color:red;"}.  
  
+ Build many other bootstrap samples of size N.

+ It can be shown that approximately 1/3 of the data [(36.8%) will be excluded from any particular bootstrap sample]{style="color:red;"}.
  
+ Build a model on the bootstrap sample and evaluate/validate on the 1/3 that is left out. Repeat on the other resamples.
  
## The Actual Process as Text {.smaller}

1.  Collect data
2.  Data exploration and preparation
    -   Cleaning
    -   Many algorithms need standardization/normalization
3.  Split the data into a [training]{style="color:red;"} and [test]{style="color:red;"} sub sets
4.  Split the training data into [analysis]{style="color:red;"}/[assessment]{style="color:red;"} sets
5.  Pick an algorithm
6.  Training using the algorithm
7.  Model evaluation
    -   RMSE or contingency table statistics (accuracy, sensitivity, specificity)
8.  Model improvement
    -   Tweak preparation, reparametrize a method or use a different method
9.  Evaluate/Test results on new data

## The Actual Process as an Image


```{r, echo = FALSE, fig.cap=""}
knitr::include_graphics("https://bradleyboehmke.github.io/HOML/images/modeling_process.png")
```

::: {.footer style="font-size: .2em;"}  
Image from: https://bradleyboehmke.github.io/HOML/images/modeling_process.png
:::

## The Steps With R Tidymodels

::: {.scroll-container style="overflow-y: scroll; height: 600px;"}
![](images/the_big_picture.jpg)
:::

## So Many Methods ... So Little Time

Once your data has been split you can use use ***many*** algorithms to try to make predictions.

"Hands-On Machine Learning with R" is one of my favorite books. It was published in 2020 so it is out of date.

Book: <https://bradleyboehmke.github.io/HOML/>

Supplemental Material: <https://koalaverse.github.io/homlr/>

GitHub for Supplemental Material: <https://github.com/koalaverse/homlr/>

   
# k Nearest Neighbors
Prediction without any abstraction/learning

## kNN
+ "Plot" your data in D dimensional space (where D is the number of predictors you have).
+ Add your new person to the plot and look at who is close by.
+ There is [no abstraction]{style="color:red;"} here.
+ Great basic coverage (but with dated code) in: 

![](images/MLwR_cover.jpg){fig-align="center" width=50%}

## Is a tomato a fruit or a vegetable? 

::: {.smaller}

> A fruit is a seed-bearing structure that develops from the ovary of a flowering plant, whereas vegetables are all other plant parts, such as roots, leaves and stems.
> 
> <https://www.livescience.com/33991-difference-fruits-vegetables.html>

:::

## kNN in Pictures

::: {layout-ncol=1}

![](images/tomato_picture1.jpg){width=30%}
![](images/tomato_picture2.jpg){width=30%}
![](images/tomato_picture3.jpg){width=30%}

:::

::: {.footer style="font-size: .2em;"}

Images from: Machine Learning with R: Expert techniques for predictive modeling to solve all your data analysis problems, 2nd Edition. Copyright 2015 Packt Publishing

:::

## The Math

![](images/euclid.png){fig-align="center" width=50%}

The kNN function will [calculate all these distances, check the class on the k closest]{style="color:red;"} foods, and report the most common class.


## Units/Scales of Measurement
+ Here the sweetness and crunchiness were on a 1 to 10 point scale.
+ What happens if we use spiciness/heat instead of sweetness?
+  **Scoville Heat Units** (SHU) based on the concentration of capsaicinoids:

![:scale 50%](images/scoville.png)

+ Nothing else will matter because the range is huge.


## Normalize or Standardize 


:::: {.columns}

::: {.column width="50%"}
+ Normalize
  + Force the range between 0 and 1.  
  + New extreme values break the normalization.
  
  $X_{new} = \frac{x-min(X)}{max(X)-min(X)}$
:::

::: {.column width="50%"}
+ Z score standardize
  + Most values will be within 3 SD.
  + New extreme values are okay.
  
  <br/>
  $X_{new} = \frac{X-\mu}{\sigma} = \frac{X - Mean(X)}{StdDev(X)}$
:::
::::



## Categorical Predictors

+ Binary 
  + No = 0, Yes = 1
  + Works well if other values are normalized
  
+ Many nominal levels
  + Dummy code / One Hot Encoding
  + Works well if other values are normalized
  
+ Ordinal data
  + Dummy code
  + If you think differences between levels are about the same, treat them as interval data and code as 1, 2, 3. Then normalize or standardize.

## The `recipies` Package Saves the Day

You can add steps to the recipe to deal with all these issues.

```{r eval=FALSE, echo=TRUE}
recipe(outcome ~ ., data = theData) %>%
  step_dummy(all_nominal()) %>%  # dummy code all character and factor variables
  step_zv(all_predictors()) %>%  # drop numeric variables with zero variance
  step_normalize(all_predictors())  # normalize the predictors
  step_impute_median(all_numeric(), -all_outcomes())  # guess missing values
  step_impute_mode(all_nominal(), -all_outcomes())  # guess missing values
```

## What pre-processing is appropriate for KNN?

```{r, cache=FALSE, echo=FALSE, fig.align="center", fig.cap="Steps to include in your recipe"}
knitr::include_graphics("images/knnPreprocessing.png", error = FALSE)
```

The full table with useful tips is [here](https://www.tmwr.org/pre-proc-table.html).

## Choose a k

+ If k = 1, and there is a typo for the class of the closest food, you are in trouble.

+ If k = n, you will always guess the most common class.

+ Typically, people choose k equal to either:
  + Square root of n, or
  + Try many values, starting at k = 3, and look at how the model does on the validation (which has the truth):
      + Accuracy
      + Precision (what epidemiology folks call PPV) 
      + Recall (what everybody else calls sensitivity)

# Doing k Nearest Neighbors


## Predict Breast Cancer with kNN


```{r eval=TRUE, message=FALSE}
wbcd <- 
  readr::read_csv("wisc_bc_data.csv")
```

```{r, eval=FALSE, message=FALSE}
URL <- 
  paste0(
    "https://raw.githubusercontent.com/dataspelunking/MLwR/master/",
    "Machine%20Learning%20with%20R%20(3rd%20Ed.)/Chapter03/wisc_bc_data.csv"
  )

wbcd <- 
  suppressMessages(readr::read_csv(URL))

write_csv(wbcd, "./06_tuning_knn/lecture/wisc_bc_data.csv")
```

```{r}
#| echo: true
#| results: hold

wbcd <- wbcd %>% 
  mutate(diagnosis = factor(diagnosis, 
                            levels = c("B", "M"), 
                            labels = c("Benign", "Malignant")))

library(janitor)

the_table <- wbcd %>% 
  tabyl(diagnosis) %>% 
  adorn_pct_formatting(0)

the_table


```


## Look

```{r}
#| echo: true
#| eval: false

library(skimr)
skim(wbcd)
```

:::{.scrolling style="font-size: .5em;"}

```{r}
#| echo: false
library(skimr)
skim(wbcd)
```

:::

## Drop Stuff... the SE Variables

```{r}
wbcd <- wbcd %>% 
  select(-ends_with("se"))

glimpse(wbcd)
```

##  Training and Test

```{r}
#| echo: true

suppressPackageStartupMessages(library(tidymodels))
set.seed(321)

# Put 3/4 of the data into the training set. Try to have same percentage benign
data_split <- 
  initial_split(wbcd,
                prop = 3/4,  # this is the default
                strata = diagnosis)

# Create training and test data sets:
train_data <- training(data_split) 
test_data <- testing(data_split)
```

## How did it do?
.pull-left[
```{r}
wbcd %>% 
  tabyl(diagnosis) %>% 
  adorn_pct_formatting(0)
```
]
.pull-right[
```{r}
train_data %>% 
  tabyl(diagnosis) %>% 
  adorn_pct_formatting(0)
```

```{r}
test_data %>% 
  tabyl(diagnosis) %>% 
  adorn_pct_formatting(0)
```
]


## Build the Recipe


```{r}
the_recipe <- 
  recipe(diagnosis ~ .,
         data = train_data) %>%
  update_role(id, 
              new_role = "ID") %>% 
  step_normalize(all_numeric(), -id) %>% 
  step_corr(all_predictors(), threshold = 0.7, method = "spearman")

the_recipe
```

## Build a Model Specfication (with Defaults)

```{r}
library(kknn)  # there is a bug so this package does not automatically load
the_model <- nearest_neighbor() %>%  # the default is to use 5 neighbors
  set_engine("kknn") %>% 
  set_mode("classification") 

the_model
```

## Specify a kNN model

```{r, eval = FALSE}
library(kknn)  
the_model <- nearest_neighbor(`neighbors = tune()`) %>%
  set_engine("kknn") %>% 
  set_mode("classification") 

the_model
```

```{r knn-model, echo=FALSE}
library(kknn)  # there is a bug so this package does not automatically load
the_model <- nearest_neighbor(neighbors = tune()) %>%
  set_engine("kknn") %>% 
  set_mode("classification") 

the_model
```

## Make the Workflow

```{r}
the_workflow <- # new workflow object
 workflow() %>% # use workflow function
 add_recipe(the_recipe) %>%   # use the new recipe
 add_model(the_model)    # add your model spec
  
the_workflow
```

## Make a Cross Validation Object

I need to tell it how to make the validation data. I will split the data into fifths.  In each 1/5 of the data, it uses 80% of the data to do the analysis and 20% to assess/evaluate.  I will get estimates that are averaged over the fifths. 

```{r}
cv_folds <-
  vfold_cv(train_data, 
           v = 5,  # the number of sets
           strata = diagnosis)  # keep the percentage with cancer the same
```

## Tune with a Simple (Slow) Grid Search
Here I am doing the kNN modeling and only checking for 3, 8, 13, 18, etc. neighbors.

```{r}
# spec is defined in readr and yardstick
suppressMessages(conflict_prefer("spec", "yardstick"))

doParallel::registerDoParallel()  # use multiple CPU cores for faster processing

the_tuned <-
  the_workflow %>% 
  tune_grid(
    resamples = cv_folds,  # data used for cross validating
    grid = data.frame(neighbors = seq(3, 53, by = 5)),
    metrics = metric_set(
      roc_auc, recall, precision, f_meas, accuracy, kap, sens, spec
    )
  )

doParallel::stopImplicitCluster()  # stop parallel processing
```

## Classification Metrics in `yardstick`

```{r echo = FALSE, warning=FALSE}
tab <- matrix(c("A", "B", "C"," D"), byrow = T, nrow = 2)

the_table <- tibble(`Predicted` = c("Yes", "No"), 
                      Yes = tab[,1], No = tab[,2])

library(flextable)
my_header <- data.frame(
  col_keys = colnames(the_table),
  line1 = c("Predicted", rep("Truth", 2)),
  line2 = colnames(the_table)
)

flextable(the_table, col_keys = my_header$col_keys) %>%
  set_header_df(
    mapping = my_header,
    key = "col_keys"
  ) %>% 
  theme_booktabs() %>% 
  autofit(part = "all") %>%    
  align(align = "center", part = "all") %>% 
  merge_h(part = "header") %>% 
  merge_v(part = "header") %>% 
  merge_h(part = "body") %>% 
  merge_v(part = "body") %>%
  align_nottext_col(align = "center") 
```

$Prevalence = (A + C) / (A + B + C + D)$

::: {style="font-size: .5em;"}

| Metric | Measures |
|--------|----------| 
| accuracy | proportion of the data that are predicted correctly |
| bal_accuracy() | average of sens() and spec() |
| detection_prevalence() | predicted positive events (both true positive and false positive) divided by the total number of predictions |
| f_meas() | $(1+\beta^2)∗percision\times recall/((\beta^2\times percision)+recall)$|
| j_index() | Youden's J $sens() + spec() - 1$ |
| kap()| kappa - performance in accuracy beyond that would be expected by chance alone |
| mcc()| Matthews correlation coefficient |
| npv()| Negative predictive value $(Specificity*(1-Prevalence))/(((1-Sensitivity)\times Prevalence) + ((Specificity) \times (1-Prevalence)))$|
| ppv()| Positive predicted value $(Sensitivity \times Prevalence)/((Sensitivity \times Prevalence) + ((1-Specificity) \times (1-Prevalence)))$|
| precision | $(A / A + B)$ |
| recall() | $A / (A + C)$ |
| sens() | $A / (A + C)$ |
| spec() | $D / (B + D)$ |

:::

## Set Parameter Limits for a Better Search
  
Here I am specifying details for a Bayesian (fast) search.  
  
```{r parameters}  
the_parameters <-   
  the_workflow %>% 
  extract_parameter_set_dials() %>% 
  update(
    neighbors = neighbors(c(3, 53)) # limits on # of neighbors to check
  ) 

class(the_parameters)
the_parameters
# limits are buried in the object 
the_parameters[[6]][[1]][["range"]][["lower"]]
the_parameters[[6]][[1]][["range"]][["upper"]]
```


## Tune with a Bayesian Homing Algorithm

```{r}
# I want to look at specificity and readr also uses a spec() function
suppressMessages(conflict_prefer("spec", "yardstick") )  

ctrl <- control_bayes(verbose = TRUE,  # show progress as it searches
                      save_pred = TRUE)  # save the predicted probabilities 
set.seed(890)

doParallel::registerDoParallel()  # use multiple cores for faster processing

the_tuned <- tune_bayes(the_workflow, 
                        resamples = cv_folds,  # cross validation details
                        initial = 10,  # number of initial tries  
                        iter = 20,  # max number of search iterations
                        param_info = the_parameters,  # object with limits
                        # save many evaluation metrics 
                        metrics = metric_set(roc_auc, accuracy, kap),
                        control = ctrl)

doParallel::stopImplicitCluster()  # stop parallel processing
```



## The Tuned Model Object

```{r}
class(the_tuned)

the_tuned
```

## How did we do? $_1$

```{r}
#| echo: true
collect_metrics(the_tuned)
```

## How did we do? $_2$

```{r}
#| echo: true
show_best(the_tuned, metric = "roc_auc")
```

## How did we do? $_3$

```{r}
#| echo: true
autoplot(the_tuned)
```

## The Best *k*

```{r}
#| echo: true
the_best <- the_tuned %>%
  select_best("roc_auc")

the_best
```

## Update the Workflow with the Best *k*

```{r}
#| echo: true
the_final_workflow <- 
  the_workflow %>% 
  finalize_workflow(the_best)

the_final_workflow
```

## Are we done?

If you want to explore the [training]{style="color:red;"} results

```{r}
#| echo: true
the_results <-
  the_final_workflow %>% 
  fit_resamples(
    resamples = cv_folds, 
    metrics = metric_set(
      recall, precision, f_meas, accuracy, kap, roc_auc, sens, spec
      ),
      control = control_resamples(save_pred = TRUE)
  ) 
```

When you are done, you can fit the model to the [test]{style="color:red;"} data
```{r}
#| echo: true
the_results <-
  the_final_workflow %>% 
  # rebuild the model using all training data (not resampled) and fit on test
  last_fit(
    split = data_split, 
    metrics = metric_set(
      recall, precision, f_meas, accuracy, kap, roc_auc, sens, spec
    )
  )
```


## Look at the Results Per Person

```{r}
the_prediction <-  
  the_results %>% 
  collect_predictions()

head(the_prediction, n = 20)
```

## Confusion Matrix

```{r}
#| echo: true
the_prediction %>% 
  conf_mat(diagnosis, .pred_class)
```

## ROC Curve

```{r}
#| echo: true
#| fig-height: 6
#| warning: false
the_prediction %>% 
  roc_curve(diagnosis, .pred_Benign) %>% 
  autoplot()
```

## Prediction Probabilities

```{r}
#| echo: true
#| eval: false
the_prediction %>% 
  ggplot() +
  geom_density(aes(x = .pred_Benign, 
                   fill = diagnosis), 
               alpha = 0.5) +
  labs(title = "True Diagnosis vs Prediction Probabilty",
       x = "Probabilty of Being Benign",
       y = "Density") +
  ggthemes::theme_few()  +
  theme(legend.title=element_blank())
```

```{r}
#| echo: false
#| fig-height: 4
the_prediction %>% 
  ggplot() +
  geom_density(aes(x = .pred_Benign, 
                   fill = diagnosis), 
               alpha = 0.5) +
  labs(title = "True Diagnosis vs Prediction Probabilty",
       x = "Probabilty of Being Benign",
       y = "Density") +
  ggthemes::theme_few()  +
  theme(legend.title=element_blank())

```


## Look at the Metrics

```{r}
#| echo: true
the_results %>%
  collect_metrics(summarize = TRUE)
```
  
## Beyond Acuracy ...

It is important to look at things beyond Yes/No, I got it right!
  + Down Syndrome happens in about 1/700 pregnancies.
  + Given 100,000 births, if your sample has 147 cases of Down....


![](images/beyondTable.png){fig-align="center" width=50%}

... and if your diagnostic test says no Down every time, you are right 99.8% of the time (accuracy = 99,883 of 100000 are right ) but your test is valueless.

## Kappa $\kappa$

Kappa measures how good you are relative to chance guessing.

::::: {.columns}

:::: {.column style="text-align: center;"}

![](images/beyondTable.png){fig-align="center" width=50%}


Accuracy = 99.853  
Kappa = 0

::::

:::: {.column style="text-align: center;"}

![](images/beyondTable2.png){fig-align="center" width=50%}

Accuracy  = 99.996  
Kappa = .986

::::

:::::


::: {style="text-align: center;"}
Very rough interpretations:  
$\kappa$ > 0.75 as excellent,   
$\kappa$ 0.40 to 0.75 as fair to good,   
$\kappa$ < 0.40 as poor.

:::

## The Answer

+ A reviewer will expect to see the ROC curve with the c statistic (what the output calls "roc_auc"), accuracy and kappa. 
+ If you are talking to epidemiologists, they will expect to see sensitivity and specificity plus negative and positive predictive values.

This is a fantastic model.  We have no idea which predictors matter....


# Linear Model Selection and Regularization

## This Section
+ Understand traditional penalty methods
+ Understand what is shrinkage
+ Understand why it is useful
+ Understand what is the difference between ridge and LASSO

## Traditional Generalized Linear Regresion Methods

+ Pro(s) 
  + Support (relatively simple) inference.
  + Surprisingly competitive compared to the state of the art.
  + They work when the sample size (N) is far more than the number of predictors ( $N \gg p$ )
+ Con(s) (Why use another fitting method?)
  + They do not work well when N is not much bigger than p
  + They do not work when you have more predictors then the sample size ( $p > N$ )
  + Variable selection is tricky.

## Old Fassion Variable Selection for Linear Models

Adding in new predictors (even noise) *will* improve R-squared.  Here are traditional ways to deal with the overfitting:

+ **Best Subset Selection**: innumerate [all possible models and cross validate]{style="color:red;"} using penalties for adding in more variables $C_p$ or AIC, BIC, or adjusted $R^2$ . 
  + That quickly gets out of hand because you need to build $2^p$ models. 
      + $p = 10$ approximately 1,000 possible models to be considered
      + $p = 20$ leads to more than 1,000,000 models.
+ **Forward or backward selection**: [using cross-validated prediction error]{style="color:red;"} to find smallest RSS or highest $R^2$ when you add or remove a predictor.
  + While these methods can lead to good predictions the $p$-values are nonsensical.

## How to choose the best? $_1$

You can add a penalty to your measure of model performance for every variable (or level of a categorical variable) that you add.
$$C_p = \frac{1}{n}(\text{RSS}+2d\hat\sigma^2)$$
where $\hat\sigma^2$ is an estimate of the variance of the error $\epsilon$ . The $C_p$ statistic adds a penalty of $2d\hat\sigma^2$ to the training RSS in order to adjust for the fact that the training error tends to underestimate the test error.

$C_p$ and AIC are proportional to each other.

## How to choose the best? $_2$

$$BIC = \frac{1}{n}(\text{RSS}+\text{log}(n)2d\hat\sigma^2)$$

The BIC statistic generally places a heavier penalty on models with many
variables, and hence results in the selection of smaller models than $C_p$.

$$\text{Adjusted } R^2 = 1 − \frac{\text{RSS}/(n − d − 1)}{\text{TSS}/(n − 1)}$$
Where TSS = $(y_i − \bar{y})^2$ is the *total sum of squares* for the response.  The adjusted $R^2$ is not as well motivated in statistical theory as AIC, BIC, and $C_p$.

I tend to use AIC (if I want more predictors) or BIC (if I want less).

## Different Metrics Give Different Results
Notice $C_p$ (AIC) is more liberal than BIC.

```{r, cache=FALSE, echo=FALSE, fig.align="center", fig.cap="", out.width = "800px"}
knitr::include_graphics("images/fig6_2.jpg", error = FALSE)
```

::: {.footer style="font-size: .2em;"}  
Fig 6.2 from ISLR
:::

## What is optimized?

+ Remember that in ordinary least squares regression or generalized linear models like logistic regression your goal is to minimize the residual sum of squared errors.  

+ The traditional optimization methods do not directly include in penalties to prevent too many predictors.

## Shrinkage
+ Shrinkage methods are an alternative to least squares which constrains or regularizes the coefficient estimates.
  + In English.... they shrinks the coefficient estimates (βs) towards zero.
+ In other words, with shrinkage algorithms you try to optimize the RSS at the same time you try to [minimize the size of the β estimates]{style="color:red;"}.
+ The two best-known techniques for shrinking the regression coefficients towards zero are ridge regression and LASSO (least absolute shrinkage and selection operator).

## Ridge vs. Lasso

$$\text{RSS} = \sum_{i=1}^n\bigg( y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij}\bigg)^2.$$
[Ridge]{style="color:red;"} ( $\ell_2$ norm) minimizes the RSS and also the [square]{style="color:red;"} of the beta estimates.
$$\sum_{i=1}^n\bigg( y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij}\bigg)^2 + \lambda \sum_{j=1}^p \beta_j^2 = \text{RSS} + \lambda \sum_{j=1}^p \beta_j^2$$
[LASSO]{style="color:red;"} ( $\ell_1$ norm) minimizes the RSS and also the [absolute value]{style="color:red;"} of the beta estimates
$$\sum_{i=1}^n\bigg( y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij}\bigg)^2 + \lambda \sum_{j=1}^p | \beta_j| = \text{RSS} + \lambda \sum_{j=1}^p | \beta_j|,$$

## What are the practicle differences?

+ Ridge shrinks all the $\beta$s by (approximately) the same fixed percentage.
  + So betas get smaller but all variables remain.
+ LASSO shrinks all the $\beta$s by (approximately) the same fixed amount.  
  + So small beta's can shrink to zero.
  + It does variable selection
  
## How do you shink?

+ You specify a range of shrinkage values and check the performance on your analysis metric (typically RMSE if your outcome is continuous or ROC AUC if your outcome is categorical) with cross validation or with bootstrap resamples.
+ You want to check values from the range of no shrinkage (which will be the same as regression) to so much shrinkage you have no predictors left (for LASSO).
  + With each amount of shrinkage you chec your goodness metric.

```{r, cache=FALSE, echo=FALSE, fig.align="center", fig.cap=""}
knitr::include_graphics("images/fig6_6.jpg", error = FALSE)
```

::: {.footer style="font-size: .2em;"}  
Fig 6.6 from ISLR 
:::

## Do you need to choose between LASSO and ridge methods?

+ *NO!*
+ It is possible to do both LASSO and ridge regression.  It is called Elastic Net Regularization.
+ I typically just do LASSO because I want the variable selection that comes from LASSO.

# CART

## Learning Objectives

+ Understand how CART methods work
+ Understand the strengths and weakness of CART
+ Understand how to interpret a classification tree
+ Understand tree pruning
+ Know how to build a classification tree
+ Know how to evaluate classification tree performance
+ Understand what to read to learn more about tree methods



```{r pretty-tibbles, echo = FALSE, results='asis'}
options(crayon.enabled = FALSE)
old.hooks <- fansi::set_knit_hooks(knitr::knit_hooks)
```

## You Want to Read This!

:::: {.columns}

::: {.column width="50%"}
![:scale 70%](images\greenwell.png)]{fig-align="center" width=50%}
:::

::: {.column width="50%"}
+ It was published in 2022.
+ It shows how the algorithms work and has superb writing.
+ It currently is missing Tidymodels and Julia code but my wife's current husband wants to fix that.
+ <https://www.amazon.com/Tree-Based-Methods-Statistical-Learning-Chapman/dp/0367532468>
:::

::::


## Classification Trees

+ Use a set of binary splits across all your variables to make a binary decision.  Start at the root and build branches until you get to leaf nodes.
+ It recursively splits the data.
+ Yes... it is drawn up-side down.

![](images\hill0.png)]{fig-align="center" width=68%}


::: {.footer style="font-size: .2em;"}
Image from: https://tmv.netlify.app/site/slides/rmed03-tune.html#4
:::

## Classification Tree Terms

![](images\hill.png)]

::: {.footer style="font-size: .2em;"}  
https://tmv.netlify.app/site/slides/rmed03-tune.html#8
:::

## Decision Trees

+ Say you want to classify a set of movies so each film falls into one of three categories: Critical Success, Mainstream Hit, Box Office Bust
![](images\movie1.png)]{fig-align="center" width=68%}

::: {.footer style="font-size: .2em;"}  
Image from Machine Learning with R: Expert techniques for predictive modeling by Brett Lantz 
:::

## How It Works

+ The algorithm will [first]{style="color:red;"} find the [best split]{style="color:red;"} in the [best predictor]{style="color:red;"}.  
  + It wants to end up with "pure" groups but it can only make splits parallel to an axis.
+ Then it does the 2nd best split ... and 3rd ... and 4th, etc. ...

:::: {.columns}

::: {.column width="50%"}
![](images\movie2.png){fig-align="center" width=48%}
![](images\movie3.png){fig-align="center" width=48%}
:::

::: {.column width="50%"}
![:scale 75%](images\movieTree.png)
:::
::::

::: {.footer style="font-size: .2em;"}  
Images from Machine Learning with R: Expert techniques for predictive modeling by Brett Lantz
:::

## Stop!

+ Try to split on [every category]{style="color:red;"} in every categorical variable and [every possible split]{style="color:red;"} on every numeric variable. Choose the "best" across all possible splits on the possible variables.

+ Keep splitting until
  + All (or nearly all) of the examples in a subgroup have the same class.
  + There are no remaining features to distinguish among the examples.
  + The tree has grown to a predefined size limit.
 
## Pruning a Decision Tree

+ If too large, decisions will be overly specific (overfitted to the training data). 
  
  + "Early stopping" or "Pre-pruning"
      + Stop at a specified number of decisions
      + Stop if there is a small number of examples
      + With an early stop, there is no way to know whether the tree will miss subtle, but important patterns.  
  
  + "Post-pruning"
      + Intentionally grow a huge, too-large tree and prune leaf nodes to reduce the size of the tree.
      + It allows the algorithm to be certain that all the important data structures were discovered.
  
## Splitting Algorithms – What is Best?

+ A bunch of different algorithms can be used to define "best".
  + Try to find splits that give you a homogeneous subgroup.
+ CART – uses [Gini Impurity]{style="color:red;"}
  + How often is a randomly chosen element from the set incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset?
  + Popular in statistics
+ Iterative Dichotomiser 3 (ID3), C4.5 (and also J48), C5.0 use [information gain]{style="color:red;"} (based on [Entropy]{style="color:red;"})
  + Popular in computer science
+ Other options include: Chi-Squared statistics, and gain ratio

## Gini and Gain for a Binary Outcome

CART checks the [Gini impurity]{style="color:red;"} of the outcome (i.e., how homogeneous is the outcome or what percentage of people have a "yes" outcome) in a node. 

```{r}
gini <- function(y) { # y should be coded as 0/1
  p <- mean(y) # proportion of successes (or 1s)
  2 * p * (1 - p) # Gini index
}
```

It then makes a binary split based on one of the predictors.  It checks how homogeneous the outcome groups are.  Repeat for *every* possible split in *every* predictor variable. Pick the split that results in the largest reduction in impurity.

Gain is the degree to which the two resulting child nodes reduce the impurity of the parent node.  CART uses whichever split results in the greatest gain.


## Variable Importance

+ The first split (top of the tree) is, in some sense, the most important because it results in the purist groups (using only one split).
+ But.... variables can be used to split repeatedly.
  + So, a different variable may result in a larger [total gain]{style="color:red;"} in purity.
+ CART trees use [surrogate variables]{style="color:red;"} to deal with missing values.
  + So a variable that is not in the tree at all may have a non-zero variable importance.

## Surrogate Variables

+ After building a tree, you will want to predict new people.
+ If the new person is missing a variable in the tree, CART will use whatever other split (in any other variable) best predicts the missing split information.
+ So, CART can handle some missing predictors.

## Special Benefits of CART

+ CART can change its prediction algorithm (weights) to avoid either false positives or false negatives.
  + Remember that different types of outcome misclassification mistakes are not the same.  
  + A false positive for cancer, HIV, Down syndrome is **bad** but it can be corrected with a second test. However, a false negative mistake is **extremely bad**.
+ CART can adjust for over- and under-sampled outcome groups. 
  + If you have a sample of blood biomarkers for breast cancer and half of the samples are cancerous, the tree will not give the correct guesses when applied in the general population, where breast cancer is relatively rare.

## A Weakness With CART - Cardinality Bias 

+ CART is biased toward variables that have many distinct values (high cardinality).
  + Remember, it is going to try *every possible* split.

:::: {.columns}

::: {.column width="50%"}
+ ch2: chi-square 
+ m2: a random factor with 2 equiprobable (RFE) categories;
+ m4: RFE 4 categories;
+ m10: RFE 10 categories;
+ m20: RFE 20 categories;
+ nor: an N (0, 1) random variable;
+ uni: a U (0, 1) random variable.
:::

::: {.column width="50%"}
![](images\fig3_1.png)
:::
::::

::: {.footer style="font-size: .2em;"}  
Tree-Based Methods for Statistical Learning in R  by Brandon M. Greenwell 
:::


## Dealing with Cardinality Bias

+ There are very modern (cutting edge) algorithms that attempt to deal with the Cardinality Bias.
  + Conditional inference trees - implemented in the .blue[`ctree`] package 
      + uses adjusted statistical tests to separately determine the split variable and split point at each node (CART just uses an exhaustive search)
      + provides unbiased split variable selection
      + does not require pruning (or much tuning)
  + GUIDE - not implemented in R (or Python or Julia)

## Predict Breast Cancer with Trees

```{r eval=TRUE, message=FALSE, warning=FALSE}
wbcd <- 
  suppressMessages(readr::read_csv("wisc_bc_data.csv")) %>% 
  mutate(diagnosis = factor(diagnosis, 
                            levels = c("B", "M"), 
                            labels = c("Benign", "Malignant"))) %>% 
  select(-id)
```

```{r, eval=FALSE, message=FALSE}
#| echo: true
wbcd <-readr::read_csv("https://raw.githubusercontent.com/dataspelunking/MLwR/master/Machine%20Learning%20with%20R%20(3rd%20Ed.)/Chapter03/wisc_bc_data.csv")

wbcd <- wbcd %>% 
  mutate(diagnosis = factor(diagnosis, 
                            levels = c("B", "M"), 
                            labels = c("Benign", "Malignant"))) %>% 
  select(-id)

```

## Training and Test

```{r}
#| echo: true
set.seed(321)

# Put 3/4 of the data into the training set. Try to have same percentage benign
data_split <- 
  initial_split(wbcd,
                prop = 3/4,  # this is the default
                strata = diagnosis)

# Create training and test data sets:
train_data <- training(data_split) 
test_data <- testing(data_split)
```


## Make a Cross Validate Object

I need to tell it how to make the validation data. I will split the data into fifths.  Build models with 80% of the data and check with the remaining 20% for validation. I am using $20^{ths}$ for speed. 

```{r}
#| echo: true
cv_folds <-
  vfold_cv(train_data, 
           v = 5,  # the number of sets
           strata = diagnosis)  # keep the percentage with cancer the same
```

## Build the Recipe

```{r}
#| echo: true
the_recipe <- 
  recipe(diagnosis ~ .,
         data = train_data) %>%
  step_normalize(all_numeric()) %>% 
  step_corr(all_predictors(), threshold = 0.7, method = "spearman")

the_recipe
```


## Cross Validation to Figure Out the Complexity Parameter with `{tidymodels}`

This is just like you have seen.

```{r , fig.height=3, message=FALSE}
#| echo: true
cart_model <- 
  decision_tree(cost_complexity = tune()) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")

cart_workflow <- 
  workflow() %>% 
  add_recipe(the_recipe) %>% 
  add_model(cart_model)
```

## Find the Best $C_p$

This is just like you have seen.

```{r}
#| echo: true
#| cache: true
doParallel::registerDoParallel()  # use multiple CPU cores for faster processing

cart_tune <- 
  cart_workflow %>% 
  tune_grid(resamples = cv_folds,
            grid = 10, 
            metrics = metric_set(roc_auc, kap),
  control = control_grid(save_pred = TRUE)
)

doParallel::stopImplicitCluster()  # stop parallel processing
```


```{r}
grid <- grid_regular(cost_complexity(), levels = 10)
grid
```

## The Search for the Best $C_p$ $_1$

```{r best}
#| echo: true
show_best(cart_tune, metric = "roc_auc")
```

## The Search for the Best $C_p$ $_2$
```{r best-plot, fig.height=5, eval=FALSE}
#| echo: true
autoplot(cart_tune)
```


```{r , fig.height=5, echo=FALSE}
autoplot(cart_tune)
```

## Finalize
```{r last-fit}
#| echo: true

cart_best <- select_best(cart_tune, metric = "roc_auc")

cart_final_workflow <- 
  cart_workflow %>% 
  finalize_workflow(cart_best)

the_results <-
  cart_final_workflow %>% 
  # rebuild the model using all training data (not resampled) and fit on test
  last_fit(split = data_split, 
           metrics = metric_set(recall, precision, f_meas, accuracy, kap,
                                roc_auc, sens, spec))
```


## Look at the Results Per Person


```{r predict}
the_prediction <-  
  the_results %>% 
  collect_predictions()

head(the_prediction, n = 20)
```

## Confusion Matrix

```{r confusion-matrix}
#| echo: true
the_prediction %>% 
  conf_mat(diagnosis, .pred_class)
```

## ROC Curve

```{r ROC}
#| echo: true
#| fig-height: 6
#| warning: false
the_prediction %>% 
  roc_curve(diagnosis, .pred_Benign) %>% 
  autoplot()
```

## Prediction Probabilities

```{r probabilty-plot}
#| echo: true
#| eval: false
the_prediction %>% 
  ggplot() +
  geom_density(aes(x = .pred_Benign, 
                   fill = diagnosis), 
               alpha = 0.5) +
  labs(title = "True Diagnosis vs Prediction Probabilty",
       x = "Probabilty of Being Benign",
       y = "Density") +
  ggthemes::theme_few()  +
  theme(legend.title=element_blank())
```

.center[
```{r, echo=FALSE, fig.height=4}
the_prediction %>% 
  ggplot() +
  geom_density(aes(x = .pred_Benign, 
                   fill = diagnosis), 
               alpha = 0.5) +
  labs(title = "True Diagnosis vs Prediction Probabilty",
       x = "Probabilty of Being Benign",
       y = "Density") +
  ggthemes::theme_few()  +
  theme(legend.title=element_blank())

```
]

## Look at the Metrics

```{r final-metrics}
the_results %>%
  collect_metrics(summarize = TRUE)
```

## CART Can Also Do Regression Trees 

+ Fundamentally the same ideas as classification trees 
+ Use binary splits on predictors to get the best prediction
+ Best is not pure group membership but least sum of squared errors

:::: {.columns}

::: {.column width="50%"}
![](images\reg1.png)
:::

::: {.column width="50%"}
![](images\reg2.png)
:::
::::


##

:::: {.columns}

::: {.column width="50%"}
![](images\reg3.png)
:::

::: {.column width="50%"}
![](images\reg4.png)
:::
::::

## 

:::: {.columns}

::: {.column width="50%"}
![](images\reg5.png)
:::

::: {.column width="50%"}
![](images\reg6.png)
:::
::::

## The Optimal Fit

+ There is no need to keep splitting.
.center[![:scale 70%](images\reg7.png)
]

## CART

+ Split on every possible point on every possible predictor and choose the two groups  (S1 and S2) such that the overall sum of squared errors are minimized: 

$$\displaystyle \text{SSE} = \sum_{i \in S_1} (y_i - \bar{y_1})^2 + \sum_{i \in S_2} (y_i - \bar{y_2})^2$$

You will end up with a very complex tree.

## CART Complexity

+ Prune the trees back.
+ Breiman et al. (1984) suggested a cost–complexity tuning. 

$$\displaystyle \text{SSE}_{c_{p}} = \text{SSE} + c_p \times (\# \text{Terminal Nodes})$$
+ Cross validate the quality of models with various levels of $c_p$ or look to see what is "close" to the most complex tree (within 1 SE of the best model).

## C4.5, J48 or C5.0

+ Instead of minimizing the Sum of Squared Errors, you can minimize entropy....

+ The Gini index tends to prefer splits that put the most frequent class into one pure node, and the remaining classes into the other.
+ Entropy puts emphasis on balancing the class sizes in the two child nodes.
+ With two classes, Gini and entropy criteria tend to produce similar results.

## Entropy

+ Typically entropy is measured in bits. 
+ With two classes, entropy ranges from 0 to 1. 
+ For n classes, entropy ranges from 0 to $log_2(n)$. 
+ The minimum value indicates that the sample is completely homogeneous, while the maximum value indicates that the data are as diverse as possible, and no group has even a small majority.

$$\text{Entropy}(S) = \sum_{i = 1}^c -\ p_i\ log_2(p_i)$$

+ For a given segment of data $(S)$, the term $c$ refers to the number of class levels and $p_i$ refers to the proportion of values falling into class level $i$.

$\uparrow pure\ \ \ \ \ \ \text{means} \downarrow entropy$  
$\uparrow entropy\ \text{means} \downarrow pure$ 


## Entropy in R

+ If we have a partition of data with two classes: red (60 percent) and white (40 percent). Entropy:

$$\text{Entropy}(S) = \sum_{i = 1}^c -\ p_i\ log_2(p_i)$$

$$-.60 \times log_2(0.60) - 0.40 \times log_2(0.4)$$

## Entropy by Purity
```{r}
#| eval: false
#| echo: true
curve(-x * log2(x) - (1-x) * log2(1-x),
      col = "red", xlab = "x", ylab = "entropy", lwd = 4)
```

```{r curve, echo = FALSE}
curve(-x * log2(x) - (1-x) * log2(1-x),
      col = "red", xlab = "x", ylab = "entropy", lwd = 4)
```


## Entropy vs. Gini

![:scale 68%](images\fig2_4.png){fig-align="center" width=50%}

::: {.footer style="font-size: .2em;"}  
Tree-Based Methods for Statistical Learning in R, Figure 2-4
:::


## Information Gain

+ The algorithm calculates the change in entropy that would result from a split on each possible feature, [information gain]{style="color:red;"}. 
+ Compare before the split (S1) and the partitions resulting from the split(S2): [Information Gain (F) = Entropy(S1) − Entropy(S2)]{style="color:red;"}

+ After a split, the data is divided into more than one partition.
  + The function to calculate Entropy(S2) needs to consider the total entropy across all of the partitions. 
  + It does this by weighting each partition’s entropy by the proportion of records falling into the partition.

$$\text{Entropy}(S) = \sum_{i= 1}^n w_i \text{Entropy}(P_i)$$ 

## This Fits an Entropy-Based Classification Tree 
```{r C50}
#| error: false
#| echo: true

library(C50)
cancerTree <- C5.0(train_data[c(-1, -2)], train_data$diagnosis)
```

##

```{r plot-C50, fig.height=9, fig.width=17}
plot(cancerTree)
```

## Pros and Cons of CART

:::: {.columns}

::: {.column width="50%"}
**Pro(s)**

+ Small trees are [easy to interpret]{style="color:red;"}.
+ [Large data]{style="color:red;"} sets are tractable.
+ The leaves form a [natural clustering]{style="color:red;"} of the data.
+ Trees can handle [all types of data]{style="color:red;"}.
+ Trees provide [automatic variable selection]{style="color:red;"}.
+ [Missing data]{style="color:red;"} is not a problem.
+ It is completely [nonparametric]{style="color:red;"}.
:::

::: {.column width="50%"}
**Cons(s)**

+ Large trees are [hard to interpret]{style="color:red;"}.
+ CART finds locally optimal solutions so it may [miss global optimum]{style="color:red;"}.
+ Lower splits are [less accurate]{style="color:red;"}.
+ Higher order interactions make [main effects hard to interpret]{style="color:red;"}.
+ CART is [biased] toward selecting high cardinality variables.
+It uses [Step functions]{style="color:red;"}.
:::
::::


## Can you do better?

+ I love how intuitive these methods are and you can still publish with them.
+ There are other (less biased) algorithms for fitting simple trees but they have other limitations, like the inability to differentially weight false positives and negatives).
  + Take a look at .blue[{`ctree`}] and GUIDE.
+ You can use code like I showed in the CART section for getting metrics on model fit.
+ If your goal is prediction, you can do a lot better by using many trees.


# `caret` Package for CART
You will still see `caret` code for CART all over the web.

## Basic CART

```{r, fig.height=4, eval=FALSE}
library(rpart, quietly = TRUE)
library(rpart.plot, quietly = TRUE)
# the default is to build a not pruned tree
tree <- rpart(diagnosis ~., data = train_data, method = "class")
rpart.plot(tree)
```


```{r, fig.height=4, echo=FALSE}
library(rpart)
library(rpart.plot)
# the default is to build a not pruned tree
tree <- rpart(diagnosis ~., data = train_data, method = "class")
rpart.plot(tree)
```

How do you know the optimal stopping rules (complexity parameter, people per node, etc.)?  You can cross validate!

## Cross Validation for Complexity Parameter with `Caret`

The state-of-the-art before `{tidymodels}` was the `caret` package.

```{r , fig.height=3, message=FALSE, eval=FALSE}
suppressWarnings(suppressMessages(library(caret)))

set.seed(23)
caretModel <- train(
  diagnosis ~., data = train_data, method = "rpart",
  trControl = trainControl("cv", number = 10), 
  tuneLength = 10)  # try 10 complexity parameters
detach("package:caret", unload = TRUE)

plot(caretModel) # look at the complexity parameter
```



```{r , fig.height=3, message=FALSE, echo=FALSE}
suppressWarnings(suppressMessages(library(caret)))
set.seed(23)
caretModel <- train(
  diagnosis ~., data = train_data, method = "rpart",
  trControl = trainControl("cv", number = 10), 
  tuneLength = 10)  # try 10 complexity parameters
detach("package:caret", unload = TRUE)

plot(caretModel) # look at the complexity parameter
```


## Looking at the Best $C_p$

```{r}
caretModel[["results"]]
```


## Fit the Optimal Tree

:::: {.columns}

::: {.column width="50%"}
```{r rpart, eval=FALSE, fig.height=5}
tree <- rpart(diagnosis ~., 
              data = train_data, 
              method = "class", 
              control = 
                rpart.control(
                  `cp = 0`
                  )
              )

library(rpart.plot)
rpart.plot(tree)
```

+ You get an easy to understand model.
+ You can figure out the chance of failure for a new person.
+ You can easily see interactions.
:::

::: {.column width="50%"}
```{r, echo=FALSE, fig.height=5}
tree <- rpart(diagnosis ~., 
              data = train_data, 
              method = "class", 
              control = 
                rpart.control(
                  cp = 0
                  )
              )

library(rpart.plot)
rpart.plot(tree)
```
:::
::::

